import tensorflow as tf
import numpy as np

filename = r"C:\temp\data\style\glove.6B\glove.6B.50d.txt"
embedding_file = filename + '.ckpt'


def load_GloVe(filename):
    vocab = []
    embd = []
    file = open(filename, 'r', encoding="utf8")
    for line in file.readlines():
        row = line.strip().split(' ')
        vocab.append(row[0])
        embd.append(row[1:])
    print('Loaded GloVe!')
    file.close()
    return vocab, embd

def save_embedding(filename, start_of_sentence_token=None):
    vocab, embd = load_GloVe(filename)
    embedding_dim = len(embd[0])
    embedding = np.asarray(embd, dtype=np.float32)
    if start_of_sentence_token is not None:
        vocab += [start_of_sentence_token]
        smallest_value = np.min(embedding)
        start_token_vector = np.array([smallest_value]*embedding_dim).reshape((1, -1))
        embedding = np.concatenate((embedding, start_token_vector), axis=0)
    vocab_size = len(vocab)

    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),
                    trainable=False, name="WordVectors")
    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
    embedding_init = W.assign(embedding_placeholder)

    saver = tf.train.Saver({"WordVectors": W})
    with tf.Session() as sess:
        sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})
        saver.save(sess, embedding_file)
        print("Model saved in file: %s" % embedding_file)


def load_embedding_example(embedding_file):
    # we only load the embeddings to get the size of W
    vocab, embd = load_GloVe(filename)
    with open(r'C:\temp\vocab.txt','w', encoding="utf8") as f:
        f.writelines([v + '\n' for v in vocab])
    vocab_size = len(vocab)
    embedding_dim = len(embd[0])

    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),
                    trainable=False, name="WordVectors")
    run_me = tf.Print(tf.Print(W, [W], message=W.name + ":"), [tf.shape(W)], message=W.name + " shape:")

    saver = tf.train.Saver({"WordVectors": W})
    with tf.Session() as sess:
        saver.restore(sess, embedding_file)
        print("Model restored.")
        sess.run(run_me, {})

save_embedding(filename, 'START')
# load_embedding_example(embedding_file)