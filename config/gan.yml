trainer:
  number_of_epochs: 5000
#  number_of_epochs: 50
  batch_size: 100
  #  batch_size: 10
  validation_batch_size: 10
  min_generator_steps: 10
  min_discriminator_steps: 1

model:
  initial_generator_epochs: 10
#  initial_generator_epochs: 2
  encoder_hidden_states: [1500, 1000, 500]
  decoder_hidden_states: [1000, 1500]
#  learn_rate: 0.0001
  learn_rate: 0.0035
#  optimizer: 'gd'
#  optimizer: 'adam'
  optimizer: 'rmsp'
  dropout: 0.3
  discriminator_dropout: 0.3
  bidirectional_encoder: False
#  curriculum_training: True
  reconstruction_coefficient: 10.0
  semantic_distance_coefficient: 0.0001
  discriminator_coefficient: 0.00001
  minimal_accuracy_for_discriminator: 0.55
  maximal_loss_for_discriminator: 99999999.9
  discriminator_type: 'embedding'
#  discriminator_type: 'content'
#  loss_type: 'cross_entropy'
#  loss_type: 'margin1'
  loss_type: 'margin2'
#  discriminator_loss_type: 'regular'
  discriminator_loss_type: 'wasserstein'
  cell_type: 'LSTM'
  #cell_type: 'GRU'

cross_entropy_loss:
  translation_hidden_size: 1000

margin_loss1:
  random_words_size: 5
  margin_coefficient: 1.0

margin_loss2:
  random_words_size: 5
  margin: 1.0

discriminator_embedding:
  include_content_vector: True
  encoder_hidden_states: [1500]
  hidden_states: [1500]
  bidirectional: False

discriminator_content:
  hidden_states: [1500]

wasserstein_loss:
  clip_value: 0.25

embedding:
  min_word_occurrences: 2
  word_size: 200
  should_train: False

#curriculum:
#  upper_range: 5.0
#  lower_range: 3.0

sentence:
  limit: 300000
#  limit: 150
  validation_limit: 29000
#  min_length: 3
  min_length: 6
  max_length: 15
  no_of_sentences: 10
