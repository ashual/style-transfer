trainer:
  number_of_epochs: 5000
#  number_of_epochs: 50
  batch_size: 100
#  batch_size: 10
  min_generator_steps: 100
  min_discriminator_steps: 100

model:
  encoder_hidden_states: [1500, 1000, 500]
  decoder_hidden_states: [1000, 1500]
#  learn_rate: 0.0001
  learn_rate: 0.0035
#  optimizer: 'gd'
#  optimizer: 'adam'
  optimizer: 'rmsp'
  dropout: 0.1
  discriminator_dropout: 0.1
  bidirectional_encoder: False
#  curriculum_training: True
  reconstruction_coefficient: 1.0
  semantic_distance_coefficient: 1.0
  minimal_accuracy_for_discriminator: 0.2
  maximal_loss_for_discriminator: 1.5
#  discriminator_type: 'embedding'
  discriminator_type: 'content'
#  loss_type: 'cross_entropy'
#  loss_type: 'margin1'
  loss_type: 'margin2'
#  discriminator_loss_type: 'regular'
  discriminator_loss_type: 'wasserstein'

cross_entropy_loss:
  translation_hidden_size: 1000

margin_loss1:
  random_words_size: 5
  margin_coefficient: 1.0

margin_loss2:
  random_words_size: 5
  margin: 1.0

discriminator_embedding:
  encoder_hidden_states: : [1000]
  hidden_states: [500]
  bidirectional: False

discriminator_content:
  hidden_states: [1000]

wasserstein_loss:
  clip_value: 1.01

embedding:
  min_word_occurrences: 2
#  min_word_occurrences: 0
  word_size: 200
#  word_size: 50
  should_train: False
#  should_train: True

#curriculum:
#  upper_range: 5.0
#  lower_range: 3.0

sentence:
#  limit: 300000
  limit: 150
  min_length: 6
#  min_length: 3
  max_length: 15
