trainer:
  number_of_epochs: 100
#  number_of_epochs: 30
  batch_size: 100
#  batch_size: 10
  validation_batch_size: 100
#  validation_batch_size: 5
  min_generator_steps: 10
  min_discriminator_steps: 1

model:
  initial_generator_epochs: 8
#  initial_generator_epochs: 2
  encoder_hidden_states: [1500, 1000, 500]
#  encoder_hidden_states: [100]
  decoder_hidden_states: [1000, 1500]
#  decoder_hidden_states: [50]
  learn_rate: 0.0035
#  optimizer: 'gd'
#  optimizer: 'adam'
  optimizer: 'rmsp'
  dropout: 0.3
  discriminator_dropout: 0.3
  bidirectional_encoder: False
#  curriculum_training: True
  reconstruction_coefficient: 10.0
  semantic_distance_coefficient: 0.000001
  discriminator_coefficient: 0.00001
  minimal_accuracy_for_discriminator: 0.55
  maximal_loss_for_discriminator: 99999999.9
  discriminator_type: 'embedding'
#  discriminator_type: 'content'
  loss_type: 'margin2'
#  discriminator_loss_type: 'regular'
  discriminator_loss_type: 'wasserstein'
  cell_type: 'LSTM'
#  cell_type: 'GRU'

margin_loss2:
  random_words_size: 5
  margin: 1.0

discriminator_embedding:
  include_content_vector: True
  encoder_hidden_states: [1500]
#  encoder_hidden_states: [100]
  hidden_states: [1500]
#  hidden_states: [50]
  bidirectional: False

discriminator_content:
  hidden_states: [1500]

wasserstein_loss:
  clip_value: 0.125

embedding:
  min_word_occurrences: 2
  word_size: 200
  should_train: False

#curriculum:
#  upper_range: 5.0
#  lower_range: 3.0

sentence:
  limit: 300000
#  limit: 150
  validation_limit: 100
#  validation_limit: 20
  min_length: 6
#  min_length: 3
  max_length: 15
