trainer:
  number_of_epochs: 50
#  number_of_epochs: 30
  batch_size: 100
#  batch_size: 10
  min_generator_steps: 10
  min_discriminator_steps: 1
  initial_generator_epochs: 8
#  initial_generator_epochs: 2

model:
  encoder_hidden_states: [1500, 1000, 500]
#  encoder_hidden_states: [100]
  decoder_hidden_states: [1000, 1500]
#  decoder_hidden_states: [50]
  learn_rate: 0.0035
#  optimizer: 'gd'
#  optimizer: 'adam'
  optimizer: 'rmsp'
  dropout: 0.3
  discriminator_dropout: 0.3
  bidirectional_encoder: False
#  curriculum_training: True
  discriminator_coefficient: 0.00001
  minimal_accuracy_for_discriminator: 0.55
  discriminator_type: 'embedding'
#  discriminator_type: 'content'
  cell_type: 'LSTM'
#  cell_type: 'GRU'

margin_loss2:
#  random_words_size: 0
  random_words_size: 5
  margin: 1.0

discriminator_embedding:
  include_content_vector: True
  encoder_hidden_states: [1500]
#  encoder_hidden_states: [100]
  hidden_states: [1500]
#  hidden_states: [50]
  bidirectional: False

discriminator_content:
  hidden_states: [1500]
#  hidden_states: [10]

wasserstein_loss:
  clip_value: 0.125

embedding:
  min_word_occurrences: 5
  word_size: 200
  should_train: False

#curriculum:
#  upper_range: 5.0
#  lower_range: 3.0

sentence:
  limit: 300000
#  limit: 150
  min_length: 15
#  min_length: 3
  max_length: 15
